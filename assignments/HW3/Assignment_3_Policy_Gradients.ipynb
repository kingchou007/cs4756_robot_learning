{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xvWOB-EU9uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bab5e0-3b8e-4661-fb0b-880f4c6b7bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colabgymrender in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.9/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (4.65.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moviepy==0.2.3.5 in /usr/local/lib/python3.9/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from moviepy==0.2.3.5) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.9/dist-packages (from moviepy==0.2.3.5) (4.65.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.9/dist-packages (from moviepy==0.2.3.5) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from moviepy==0.2.3.5) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imageio<3.0,>=2.1.2->moviepy==0.2.3.5) (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.9/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imageio==2.4.1) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from imageio==2.4.1) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: AutoROM in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.9/dist-packages (from AutoROM) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from AutoROM) (2.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from AutoROM) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (2022.12.7)\n",
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.9/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender\n",
        "!pip install -U moviepy==0.2.3.5\n",
        "!pip install imageio==2.4.1\n",
        "!pip install --upgrade AutoROM\n",
        "!AutoROM --accept-license\n",
        "!pip install gym[classic_control] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "directory = './video'\n"
      ],
      "metadata": {
        "id": "i8VK3wqrU-xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac2f851-43dd-4e90-eaf2-d3b899758a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3366912/45929032 bytes (7.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7069696/45929032 bytes (15.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10534912/45929032 bytes (22.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14188544/45929032 bytes (30.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17776640/45929032 bytes (38.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21184512/45929032 bytes (46.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24879104/45929032 bytes (54.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28090368/45929032 bytes (61.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31662080/45929032 bytes (68.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35299328/45929032 bytes (76.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38608896/45929032 bytes (84.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42131456/45929032 bytes (91.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45662208/45929032 bytes (99.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please use `sobel` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing the Environment\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nH4ia3YaOF455jcrfXNzKxTNpMQl8gdP?usp=sharing)\n",
        "\n",
        "### CartPole-V0\n",
        "\n",
        "CartPole-V0 is a classic control problem in the field of reinforcement learning. The task is to balance a pole on a cart by moving the cart left or right. The environment consists of a cart that can move horizontally along a track, and a pole that is attached to the cart by a hinge. The pole can rotate freely around the hinge, and the goal is to keep the pole balanced by moving the cart to keep the pole upright.\n",
        "\n",
        "The observation space consists of four variables: the horizontal position and velocity of the cart, and the angle and angular velocity of the pole. The action space consists of two discrete actions: move the cart to the left, or move the cart to the right. The episode ends when the pole falls beyond a certain angle, or the cart moves too far from the center."
      ],
      "metadata": {
        "id": "MPYF9hU0B5Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env = Recorder(env, directory)\n",
        "\n",
        "obs = env.reset()\n",
        "for i in range(500):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "xyyDN6UYB36k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "08c0cded-9b0a-43ee-9a73-4c74bb1cac49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/moviepy/video/io/ffmpeg_reader.py:145: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  result = np.fromstring(s, dtype='uint8')\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.9/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file ./video/1679596678.8493419.mp4, 720000 bytes wanted but 0 bytes read,at frame 11/12, at time 0.37/0.37 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Policy Gradient\n",
        "\n",
        "In this assignment, we will implement the Policy Gradient algorithm. This algorithm is a popular reinforcement learning algorithm that can be used for both discrete and continuous action spaces. \n",
        "\n",
        "## Overview\n",
        "\n",
        "The Policy Gradient algorithm uses a neural network to learn a policy that maps states to actions. The algorithm collects a set of trajectories, which are used to update the policy network. \n",
        "\n",
        "The goal in reinforcement learning to maximize the performance, i.e. find a policy $\\pi_{\\theta}$ that induce a distribution over trajectories $P_\\theta(\\tau)$ such that when we sample trajectories $\\tau$ from it, the expected reward $r(\\tau)$ is maximized. \n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim P_{\\theta}}[r(\\tau)] $$\n",
        "\n",
        "each trajectory or rollout $\\tau$ is of length $T$ can be defined as \n",
        "\n",
        "$$ P(\\tau) = p(s_0) \\pi_θ (a_0|s_0) \\prod_{t=1}^{T-1} p (s_t|s_{t-1}, a_{t-1}) \\pi_\\theta (a_t | s_t)$$\n",
        "\n",
        "and the reward of a trajectory is\n",
        "\n",
        "$$ r(\\tau) = \\sum_{t=0}^{T-1} r(s_t, a_t)$$\n",
        "\n",
        "The policy gradient theorem gives us a gradient for the objective:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim P_{\\theta}}[\\nabla_θ \\log P_\\theta (\\tau) r(\\tau)] $$\n",
        "\n",
        "which in practice can be approximated from $N$ trajectories as follows:-\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\Biggl( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it})\\Biggl) \\Biggl(\\sum_{t=0}^{T-1} r(s_{it}, a_{it}) \\Biggl) $$\n",
        "\n",
        "where $s_{it}$ and $a_{it}$ are the states and actions of trajectory $i$ and time $t$. \n",
        "\n",
        "Multiplying returns by a discount factor γ can serve as an incentive for the agent to prioritize more immediate rewards over those in the distant future. Additionally, it can help to decrease variability by reducing the potential for variance in future outcomes. \n",
        "\n",
        "Adjusting for the discounted return the gradient, the reward function $r(\\tau)$ can be converted into this:\n",
        "\n",
        "\n",
        "$$ r(\\tau) = \\sum_{t'=0}^{T-1} \\gamma^{t'-1} r(s_{it'}, a_{it'}) $$\n",
        "\n",
        "where $r_i$ is the reward at time step $i$, and $\\gamma$ is the discount factor.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `PolicyNet` class - This class will define the policy network used in the Policy Gradient algorithm. \n",
        "\n",
        "2. `PolicyGradient` class - This class will define the Policy Gradient algorithm. You will need to modify the `train` method to collect trajectories and update the policy network using the Policy Gradient loss function. \n",
        "\n",
        "3. `evaluate` method - This method will evaluate the policy network by running multiple episodes. \n",
        "\n",
        "Follow the instructions below to implement each of these components.\n",
        "\n",
        "### `PolicyNet` class\n",
        "\n",
        "The `PolicyNet` class should define a neural network that takes in a state and outputs a probability distribution over the action space. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and `action_dim` output nodes, followed by a ReLU activation function and then a softmax activation function.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network. "
      ],
      "metadata": {
        "id": "lqGleARICNcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):\n",
        "        \"\"\"Policy network for the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            action_dim (int): Dimension of the action space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(PolicyNet, self).__init__()\n",
        "        ## TODO: Implement the policy network for the REINFORCE algorithm here\n",
        "\n",
        "    def forward(self, state: torch.Tensor):\n",
        "        \"\"\"Forward pass of the policy network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Probabilities of the actions.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the forward pass of the policy network here"
      ],
      "metadata": {
        "id": "ErpoIXHZU_nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### `PolicyGradient` class\n",
        "\n",
        "The `PolicyGradient` class should define the Policy Gradient algorithm. You will need to modify the `train` method to collect trajectories and update the policy network using the Policy Gradient loss function. \n",
        "\n",
        "The `PolicyGradient` class should have the following methods:\n",
        "\n",
        "- `__init__(self, env, policy_net: PolicyNet, reward_to_go: bool = False)`: Constructor method that initializes the environment and policy network.\n",
        "\n",
        "- `select_action(self, state)`: Method that selects an action based on the policy network.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the loss given the episode which is a list of states, actions and rewards\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, gamma)`: Method that updates the policy network\n",
        "\n",
        "- `train(self, num_outer_loop, num_episodes, gamma, lr)`: Method that trains the policy network using the Policy Gradient algorithm.\n",
        "\n",
        "- `run_episode(self)`: Method that runs an episode (list of (state, action, reward) tuples) of the environment and returns the episode.\n",
        "\n",
        "- `evaluate(self, num_episodes = 100, max_steps = 1000)`: Method that evaluates the policy network by running multiple episodes."
      ],
      "metadata": {
        "id": "-uziLFF-0KnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(self, env, policy_net: PolicyNet, reward_to_go: bool = False):\n",
        "        \"\"\"Policy gradient algorithm based on the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): Environment\n",
        "            policy_net (PolicyNet): Policy network\n",
        "            reward_to_go (bool): Whether to use returns or reward-to-go\n",
        "        \"\"\"\n",
        "        self.env = Recorder(env, directory)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.reward_to_go = reward_to_go\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action based on the policy network\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): State of the environment\n",
        "\n",
        "        Returns:\n",
        "            action (int): Action to be taken\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the action selection here based on the policy network output probabilities\n",
        "        ## Hint: Use torch.distributions.Categorical\n",
        "    \n",
        "    def compute_loss(self, episode, gamma = 0.99):\n",
        "        \"\"\"Compute the loss function for the REINFORCE algorithm\n",
        "\n",
        "        Args:\n",
        "            episode (list): List of tuples (state, action, reward). \n",
        "            gamma (float): Discount factor\n",
        "\n",
        "        Returns:\n",
        "            loss (torch.Tensor): Loss function\n",
        "        \"\"\"\n",
        "        # Extract states, actions and rewards from the episode\n",
        "        \n",
        "        # Compute the discounted returns\n",
        "        if not self.reward_to_go:\n",
        "          ## TODO: Part 1: Compute the discounted returns here\n",
        "          pass\n",
        "        else:\n",
        "          ## TODO: Part 2: Compute the discounted reward to go here\n",
        "          pass\n",
        "\n",
        "\n",
        "        ## TODO: Implement the loss function for the REINFORCE algorithm here based on the discounted returns and the log probabilities of the actions\n",
        "        loss = None\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def update_policy(self, episodes, optimizer, gamma):\n",
        "        \"\"\"Update the policy network using the batch of episodes\n",
        "\n",
        "        Args:\n",
        "            episodes (list): List of episodes\n",
        "            optimizer (torch.optim): Optimizer\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        # Compute the loss function for each episode\n",
        "        losses = None\n",
        "        \n",
        "        # Compute the gradients and update the policy network\n",
        "        pass\n",
        "    \n",
        "    def run_episode(self, render = False):\n",
        "        \"\"\"\n",
        "        Run an episode of the environment and return the episode\n",
        "        \n",
        "        Returns:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "        \"\"\"\n",
        "        state = self.env.reset()\n",
        "        episode = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        if render:\n",
        "          self.env.play()\n",
        "        return episode\n",
        "\n",
        "    def train(self, num_outer_loop, num_episodes, gamma, lr):\n",
        "        \"\"\"Train the policy network using the REINFORCE algorithm\n",
        "\n",
        "        Args:\n",
        "            num_outer_loop (int): Number of outerloops, i.e., calls to update_policy\n",
        "            num_episodes (int): Number of episodes to collect in each iteration\n",
        "            gamma (float): Discount factor\n",
        "            lr (float): Learning rate\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the training loop for the REINFORCE algorithm here \n",
        "        ## using the update_policy function to update the policy network\n",
        "    \n",
        "    def evaluate(self, num_episodes = 100, max_steps = 1000):\n",
        "        \"\"\"Evaluate the policy network by running multiple episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): Number of episodes to run\n",
        "            max_steps (int): Maximum number of steps in the episode\n",
        "        Returns:\n",
        "            average_reward (float): Average reward over the episodes\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the evaluation loop for the REINFORCE algorithm here by running multiple episodes and averaging the returns"
      ],
      "metadata": {
        "id": "JNMpHrns0Jz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "## TODO: Construct the policy network and the policy gradient algorithm here and train the policy network "
      ],
      "metadata": {
        "id": "WSQTxjO6rxKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "In this part of the assignment, you have implemented the Policy Gradient algorithm. This algorithm is a popular reinforcement learning algorithm that can be used for both discrete and continuous action spaces. \n",
        "\n",
        "You have learned how to define a policy network and how to update the policy network using the Policy Gradient loss function. You have also learned how to train and evaluate the Policy Gradient algorithm.\n",
        "\n",
        "# Part 2: Reward-to-Go\n",
        "\n",
        "In this part of the assignment, we will modify the Policy Gradient algorithm to use the reward-to-go instead of the total discounted reward. \n",
        "\n",
        "To decrease the variance of the policy gradient, one approach is to utilize causality, which means that the policy cannot influence past reward. This results in a revised objective where the total returns only include those acquired after the policy is evaluated. These returns are considered a sample estimation of the Q function and are known as the \"reward-to-go.\"\n",
        "\n",
        "## Overview\n",
        "\n",
        "The reward-to-go is defined as:\n",
        "\n",
        "Without discounting:\n",
        "$$ r(\\tau) = \\sum_{t'=t}^{T-1} r(s_{it'}, a_{it'}) $$\n",
        "\n",
        "With discounting:\n",
        "$$ r(\\tau) = \\sum_{t'=t}^{T-1} \\gamma^{t'-1} r(s_{it'}, a_{it'}) $$\n",
        "\n",
        "where $r_i$ is the reward at time step $i$. \n",
        "\n",
        "The updated policy gradient (with discounted reward-to-go) will look like this: \n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\Biggl(\\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\Biggl(\\sum_{t'=t}^{T-1} \\gamma^{t'-1} r(s_{it'}, a_{it'}) \\Biggl) \\Biggl)$$\n",
        "\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to modify the `PolicyGradient` class to use the reward-to-go instead of the total discounted return. \n",
        "\n",
        "Follow the instructions below to modify the `PolicyGradient` class:\n",
        "\n",
        "1. Modify the `train` method to compute the reward-to-go for each time step.\n",
        "\n",
        "2. Modify the `compute_loss` method to use the reward-to-go instead of the total discounted return."
      ],
      "metadata": {
        "id": "TtRRbGf_0R5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "## TODO: Construct the policy network with and the policy gradient algorithm now with the reward to go functionality here and train the policy network "
      ],
      "metadata": {
        "id": "muVO7jcQBqJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to use the space below to run experiments and plots used in your writeup."
      ],
      "metadata": {
        "id": "_q12JGt7Bw5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "In this part of the assignment, you have modified the Policy Gradient algorithm to use the reward-to-go instead of the total discounted return. \n",
        "\n",
        "You have learned how to compute the reward-to-go, and how to modify the Policy Gradient loss function to use the reward-to-go. \n",
        "\n",
        "# Part 3: Actor-Critic Policy Gradient (Extra Credit)\n",
        "\n",
        "In this part of the assignment, we will implement the Actor-Critic Policy Gradient algorithm. This algorithm is an extension to the Policy Gradient algorithm, and it combines both value-based and policy-based methods. \n",
        "\n",
        "This is another way to reduce the variance of the model by subtracting a baseline (which is constant with respect to ${\\tau}$ ) from the sum of returns. This modifies our original objective function as follows:-\n",
        "\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau) - b] $$\n",
        "\n",
        "where b is the baseline\n",
        "\n",
        "## Overview\n",
        "\n",
        "The Actor-Critic Policy Gradient algorithm consists of two networks: the policy network and the value network. The policy network is used to select actions, while the value network is used to estimate the value of a state. \n",
        "\n",
        "In this part, we will construct a value function $V_ϕ^\\pi$ that functions as a baseline that depends on the state. The purpose of this value function is to estimate the total future rewards that begin from a specific state.\n",
        "\n",
        "The value function can be estimated as follows:-\n",
        "\n",
        "$$ V_ϕ^\\pi (s_t) \\approx \\sum_{t'= t}^{T-1} \\mathbb{E}_{\\pi_\\theta} [r(s_{t'}, a_{t'})|s_t] $$\n",
        "\n",
        "\n",
        "In each episode, the algorithm collects a set of trajectories, which are used to update the policy network and the value network. The update is done using the following loss functions:\n",
        "\n",
        "$$ L_{\\text{policy}} = -\\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t|s_t) A_t $$\n",
        "\n",
        "$$ L_{\\text{value}} = \\frac{1}{2}\\sum_{t=0}^{T} (V_{\\phi}(s_t) - R_t)^2 $$\n",
        "\n",
        "where $\\pi_{\\theta}$ is the policy network, $a_t$ is the action selected at time step $t$, $s_t$ is the state at time step $t$, $A_t$ is the advantage function, $V_{\\phi}$ is the value network, and $R_t$ is the discounted return at time step $t$. \n",
        "\n",
        "The advantage function is defined as:\n",
        "\n",
        "$$ A_t = Q_t - V_{\\phi}^\\pi(s_t) $$\n",
        "\n",
        "where $Q_t$ is the estimated value of the state-action pair $(s_t, a_t)$.\n",
        "\n",
        "and the final policy gradient now looks as follows:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\Biggl( \\Biggl(\\sum_{t'=t}^{T-1} \\gamma^{t'-1} r(s_{it'}, a_{it'}) \\Biggl) - V_ϕ^\\pi (s_{it}) \\Biggl) $$\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `ValueNet` class - This class will define the value network used in the Actor-Critic algorithm. \n",
        "\n",
        "2. `ActorCriticPolicyGradient` class - This class will define the Actor-Critic Policy Gradient algorithm. You will need to modify the `PolicyGradient` class to include the value network and to compute the loss using the Actor-Critic loss functions. \n",
        "\n",
        "3. `compute_loss` method - This method will compute the policy loss and value loss for a given episode. \n",
        "\n",
        "4. `update_policy` method - This method will update the policy network and value network using a batch of episodes. \n",
        "\n",
        "5. `train` method - This method will train the policy network and value network using the Actor-Critic algorithm. \n",
        "\n",
        "6. `evaluate` method - This method will evaluate the policy network by running multiple episodes. \n",
        "\n",
        "Follow the instructions below to implement each of these components.\n",
        "\n",
        "### `ValueNet` class\n",
        "\n",
        "The `ValueNet` class should define a neural network that takes in a state and outputs an estimate of the value of that state. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and 1 output node.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network. "
      ],
      "metadata": {
        "id": "UaLnEgl60-VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, hidden_dim: int):\n",
        "        \"\"\"Value network for the Actor-Critic algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(ValueNet, self).__init__()\n",
        "        ## TODO: Implement the value network for the REINFORCE with baseline algorithm here\n",
        "\n",
        "    def forward(self, state: torch.Tensor):\n",
        "        \"\"\"Forward pass of the value network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Estimated value of the state.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the forward pass of the value network here"
      ],
      "metadata": {
        "id": "uj3ggq0DsFWp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### `ActorCriticPolicyGradient` class\n",
        "\n",
        "The `ActorCriticPolicyGradient` class should define the Actor-Critic Policy Gradient algorithm. You will need to modify the `PolicyGradient` class to include the value network and to compute the loss using the Actor-Critic loss functions. \n",
        "\n",
        "Only apply the **discounted reward-to-go** formulation here for calculating the rewards\n",
        "\n",
        "\n",
        "### `compute_loss` method\n",
        "The `compute_loss` method should compute the policy loss and value loss for a given episode. \n",
        "\n",
        "The policy loss is the negative sum of the log probabilities of the actions multiplied by the advantage function:\n",
        "\n",
        "$$ L_{\\text{policy}} = -\\sum_{t=0}^{T} \\log \\pi_{\\theta}(a_t|s_t) A_t $$\n",
        "\n",
        "where $\\pi_{\\theta}$ is the policy network, $a_t$ is the action selected at time step $t$, $s_t$ is the state at time step $t$, and $A_t$ is the advantage function.\n",
        "\n",
        "The value loss is the mean squared error between the estimated value of the state and the discounted return:\n",
        "\n",
        "$$ L_{\\text{value}} = \\frac{1}{2}\\sum_{t=0}^{T} (V_{\\phi}(s_t) - R_t)^2 $$\n",
        "\n",
        "where $V_{\\phi}$ is the value network, and $R_t$ is the return at time step $t$.\n",
        "\n",
        "You will need to modify the `compute_loss` method in the `ActorCriticPolicyGradient` class to compute these losses. "
      ],
      "metadata": {
        "id": "ijmmeweW0Wt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCriticPolicyGradient(PolicyGradient):\n",
        "    def __init__(self, env, policy_net: PolicyNet, value_net: ValueNet):\n",
        "        \"\"\"Actor-Critic policy gradient algorithm.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): Environment\n",
        "            policy_net (PolicyNet): Policy network\n",
        "            value_net (ValueNet): Value network\n",
        "        \"\"\"\n",
        "        self.env = Recorder(env, directory)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.value_net = value_net.to(self.device)\n",
        "            \n",
        "    def compute_loss(self, episode, gamma = 0.99):\n",
        "        \"\"\"Compute the loss function for the Actor-Critic algorithm\n",
        "\n",
        "        Args:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "            gamma (float): Discount factor\n",
        "\n",
        "        Returns:\n",
        "            policy_loss (torch.Tensor): Policy loss function\n",
        "            value_loss (torch.Tensor): Value loss function\n",
        "        \"\"\"\n",
        "        # Extract states, actions and rewards from the episode\n",
        "\n",
        "        # Compute the discounted reward-to-go\n",
        "\n",
        "        policy_loss = None\n",
        "        value_loss = None\n",
        "        ## TODO: Implement the loss function for the REINFORCE algorithm here based on the discounted rewards and the log probabilities of the actions\n",
        "        \n",
        "        return policy_loss, value_loss\n",
        "    \n",
        "    def update_policy(self, episodes, optimizer, value_optimizer, gamma):\n",
        "        \"\"\"Update the policy network and value network using the batch of episodes\n",
        "\n",
        "        Args:\n",
        "            episodes (list): List of episodes\n",
        "            optimizer (torch.optim): Optimizer for the policy network\n",
        "            value_optimizer (torch.optim): Optimizer for the value network\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        # TODO: Compute the loss function for each episode\n",
        "        policy_losses = None\n",
        "        value_losses = None\n",
        "        \n",
        "        # TODO: Compute the gradients and update the policy network and value network\n",
        "        pass\n",
        "\n",
        "    def train(self, num_outer_loop, num_episodes, gamma, lr):\n",
        "        \"\"\"Train the policy network using the REINFORCE algorithm\n",
        "\n",
        "        Args:\n",
        "            num_outer_loop (int): Number of outerloops i.e. iterations, i.e., calls to update_policy\n",
        "            num_episodes (int): Number of episodes to collect in each iteration\n",
        "            gamma (float): Discount factor\n",
        "            lr (float): Learning rate\n",
        "        \"\"\"\n",
        "        ## TODO: Implement the training of the policy network and value network using the Actor-Critic algorithm "
      ],
      "metadata": {
        "id": "F28lvwEo0WD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "## TODO: Construct the policy network, value networks and the policy gradient algorithm here and train the policy network and value networks"
      ],
      "metadata": {
        "id": "OheSn_AuxUiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this assignment, we have implemented three variants of the Policy Gradient algorithm: \n",
        "\n",
        "1. The vanilla Policy Gradient algorithm, which uses the total discounted return as the target.\n",
        "\n",
        "2. The Policy Gradient algorithm with reward-to-go, which uses the reward-to-go as the target.\n",
        "\n",
        "3. The Actor-Critic Policy Gradient algorithm, which combines both value-based and policy-based methods. \n",
        "\n",
        "We have learned how to implement each of these algorithms using PyTorch and OpenAI Gym. We have also learned how to modify the loss function and update the network parameters to account for the different variants of the Policy Gradient algorithm.\n",
        "\n",
        "Overall, the Policy Gradient algorithm is a powerful method for solving reinforcement learning problems where the action space is continuous and the environment is stochastic. The reward-to-go and Actor-Critic variants of the algorithm can improve performance and stability in some cases."
      ],
      "metadata": {
        "id": "SLVNX2k_0zw_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HymLiA868reN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}